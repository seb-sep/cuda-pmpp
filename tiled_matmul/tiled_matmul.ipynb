{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3, 5, 3, 4],\n",
       "         [2, 4, 7, 3],\n",
       "         [3, 5, 6, 0],\n",
       "         [7, 6, 2, 2]], dtype=torch.int32),\n",
       " tensor([[8, 9, 4, 1],\n",
       "         [1, 7, 1, 8],\n",
       "         [3, 2, 7, 4],\n",
       "         [0, 1, 8, 5]], dtype=torch.int32))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(low=0, high=10, size=(4, 4), dtype=torch.int32)\n",
    "b = torch.randint(low=0, high=10, size=(4, 4), dtype=torch.int32)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 38,  72,  70,  75],\n",
       "         [ 41,  63,  85,  77],\n",
       "         [ 47,  74,  59,  67],\n",
       "         [ 68, 111,  64,  73]], dtype=torch.int32),\n",
       " tensor([[ 38,  72,  70,  75],\n",
       "         [ 41,  63,  85,  77],\n",
       "         [ 47,  74,  59,  67],\n",
       "         [ 68, 111,  64,  73]], dtype=torch.int32))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "def tiled_matmul(a: torch.Tensor, b: torch.Tensor, tile_width = 2):\n",
    "    assert a.shape[1] == b.shape[0]\n",
    "    res = torch.zeros(a.shape[0], b.shape[1], dtype=torch.int32)\n",
    "\n",
    "    # mock shared memory\n",
    "    sma = torch.empty((tile_width, tile_width), dtype=torch.int32)\n",
    "    smb = torch.empty((tile_width, tile_width), dtype=torch.int32)\n",
    "\n",
    "    # number of tiles for outer dims\n",
    "    m_tiles = a.shape[0] // tile_width + (a.shape[0] % tile_width)\n",
    "    n_tiles = b.shape[1] // tile_width + (b.shape[1] % tile_width)\n",
    "    # inner dim tiles\n",
    "    k_tiles = a.shape[1] // tile_width + (a.shape[1] % tile_width)\n",
    "\n",
    "    # tiling coordinates\n",
    "    for i, j in product(range(0, m_tiles*tile_width, tile_width), range(0, n_tiles*tile_width, tile_width)):\n",
    "        # loop over each phase of tiled matmul\n",
    "        for p in range(k_tiles):\n",
    "\n",
    "            # each thread loads a value from a and b into shared memory\n",
    "            # a phases across a row, b phases across a column\n",
    "            for ti, tj in product(range(tile_width), repeat=2):\n",
    "                if (i+ti) < a.shape[0] and (tile_width*p+tj) < a.shape[1]:\n",
    "                    sma[ti][tj] = a[i+ti][tile_width*p+tj]\n",
    "                else:\n",
    "                    sma[ti][tj] = 0\n",
    "                if (tile_width*p+ti) < b.shape[0] and (j+tj) < b.shape[1]:\n",
    "                    smb[ti][tj] = b[tile_width*p+ti][j+tj]\n",
    "                else:\n",
    "                    smb[ti][tj] = 0\n",
    "\n",
    "            # need to do two separate loops to fully load in shared memory before dot products\n",
    "            for ti, tj in product(range(tile_width), repeat=2):\n",
    "                # dot the tith row of a with the tjth col of b\n",
    "                if (i+ti) < res.shape[0] and (j+tj) < res.shape[1]:\n",
    "                    res[i+ti][j+tj] += sum(sma[ti].flatten() * smb[:, tj].flatten()) # each thread does tile_width muls\n",
    "\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "print(torch.equal(a @ b, tiled_matmul(a, b, tile_width=2)))\n",
    "a @ b, tiled_matmul(a, b, tile_width=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in CUDA version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/seb/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...\n",
      "The input conditions for extension module m have changed. Bumping to version 4 and re-building as m_v4...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/seb/.cache/torch_extensions/py312_cu121/m/build.ninja...\n",
      "/home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module m_v4...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=m_v4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/include -isystem /home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/include/TH -isystem /home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda-12.3/include -isystem /home/seb/miniconda3/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/seb/CUDA/pmpp/tiled_matmul/main.cpp -o main.o \n",
      "[2/3] /usr/local/cuda-12.3/bin/nvcc --generate-dependencies-with-compile --dependency-output tiled_matmul.cuda.o.d -DTORCH_EXTENSION_NAME=m_v4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/include -isystem /home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/include/TH -isystem /home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda-12.3/include -isystem /home/seb/miniconda3/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 --compiler-options '-fPIC' -std=c++17 -c /home/seb/CUDA/pmpp/tiled_matmul/tiled_matmul.cu -o tiled_matmul.cuda.o \n",
      "[3/3] c++ main.o tiled_matmul.cuda.o -shared -L/home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-12.3/lib64 -lcudart -o m_v4.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module m_v4...\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.cpp_extension import load\n",
    "module = load(\n",
    "    name = 'm',\n",
    "    sources = ['main.cpp', 'tiled_matmul.cu'],\n",
    "    verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3178, 0.7526],\n",
      "        [0.0085, 0.3547],\n",
      "        [0.6547, 0.2386],\n",
      "        [0.5069, 0.3096]], device='cuda:0') tensor([[0.6467, 0.8290, 0.1769, 0.1295, 0.7825],\n",
      "        [0.3927, 0.2491, 0.2163, 0.6451, 0.7824]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((4, 2), device='cuda')\n",
    "b = torch.rand((2, 5), device='cuda')\n",
    "print(a, b)\n",
    "\n",
    "\n",
    "\n",
    "# a = torch.randint(low=0, high=10, size=(4, 4), dtype=torch.int64, device='cuda')\n",
    "# b = torch.randint(low=0, high=10, size=(4, 4), dtype=torch.int64, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5011, 0.4509, 0.2190, 0.5267, 0.8375],\n",
       "         [0.1448, 0.0954, 0.0782, 0.2299, 0.2842],\n",
       "         [0.5171, 0.6021, 0.1674, 0.2387, 0.6989],\n",
       "         [0.4494, 0.4973, 0.1567, 0.2654, 0.6388]], device='cuda:0'),\n",
       " tensor([[0.5011, 0.4509, 0.2190, 0.5267, 0.8375],\n",
       "         [0.1448, 0.0954, 0.0782, 0.2299, 0.2842],\n",
       "         [0.5171, 0.6021, 0.1674, 0.2387, 0.6989],\n",
       "         [0.4494, 0.4973, 0.1567, 0.2654, 0.6388]], device='cuda:0'))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.matmul(a, b), a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
