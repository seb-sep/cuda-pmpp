{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[8, 2, 4, 5],\n",
       "         [4, 2, 2, 6],\n",
       "         [5, 5, 4, 8],\n",
       "         [3, 6, 8, 9],\n",
       "         [5, 7, 3, 6],\n",
       "         [8, 1, 9, 9]]),\n",
       " tensor([[0, 4, 4, 6, 9, 3, 4, 1],\n",
       "         [8, 4, 4, 8, 5, 6, 1, 6],\n",
       "         [2, 5, 1, 9, 9, 9, 4, 0],\n",
       "         [6, 5, 7, 7, 9, 8, 5, 7]]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(low=0, high=10, size=(6, 4))\n",
    "b = torch.randint(low=0, high=10, size=(4, 8))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2 4\n",
      "True\n",
      "3 2 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 54,  85,  79, 135, 163, 112,  75,  55],\n",
       "         [ 56,  64,  68, 100, 118,  90,  56,  58],\n",
       "         [ 96, 100, 100, 162, 178, 145,  81,  91],\n",
       "         [118, 121, 107, 201, 210, 189,  95, 102],\n",
       "         [ 98,  93,  93, 155, 161, 132,  69,  89],\n",
       "         [ 80, 126, 108, 200, 239, 183, 114,  77]]),\n",
       " tensor([[ 54,  85,  79, 135, 163, 112,  75,  55],\n",
       "         [ 56,  64,  68, 100, 118,  90,  56,  58],\n",
       "         [ 96, 100, 100, 162, 178, 145,  81,  91],\n",
       "         [118, 121, 107, 201, 210, 189,  95, 102],\n",
       "         [ 98,  93,  93, 155, 161, 132,  69,  89],\n",
       "         [ 80, 126, 108, 200, 239, 183, 114,  77]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "def tiled_matmul(a: torch.Tensor, b: torch.Tensor, tile_width = 2):\n",
    "    assert a.shape[1] == b.shape[0]\n",
    "    res = torch.zeros(a.shape[0], b.shape[1], dtype=int)\n",
    "\n",
    "    # mock shared memory\n",
    "    sma = torch.empty((tile_width, tile_width), dtype=torch.int64)\n",
    "    smb = torch.empty((tile_width, tile_width), dtype=torch.int64)\n",
    "\n",
    "    # number of tiles for outer dims\n",
    "    m_tiles = a.shape[0] // tile_width + (a.shape[0] % tile_width)\n",
    "    n_tiles = b.shape[1] // tile_width + (b.shape[1] % tile_width)\n",
    "    # inner dim tiles\n",
    "    k_tiles = a.shape[1] // tile_width + (a.shape[1] % tile_width)\n",
    "\n",
    "    print(m_tiles, k_tiles, n_tiles)\n",
    "    \n",
    "    # tiling coordinates\n",
    "    for i, j in product(range(0, m_tiles*tile_width, tile_width), range(0, n_tiles*tile_width, tile_width)):\n",
    "        # loop over each phase of tiled matmul\n",
    "        for p in range(k_tiles):\n",
    "\n",
    "            sma.zero_(), smb.zero_()\n",
    "            # print(f'({i, tile_width*p}), ({i, tile_width*(p+1)})')\n",
    "            # print(f'({i+tile_width, tile_width*p}), ({i+tile_width, tile_width*(p+1)})')\n",
    "            # load in shared memory\n",
    "            # matrix a, we go across a row\n",
    "            # matrix b, go across column\n",
    "            ai_range = slice(i, min(i+tile_width, a.shape[0]))\n",
    "            aj_range = slice(tile_width*p, min(tile_width*(p+1), a.shape[1]))\n",
    "            bi_range = aj_range\n",
    "            bj_range = slice(j, min(j+tile_width, b.shape[1]))\n",
    "\n",
    "            sma = a[ai_range, aj_range].clone() #clone() prevents this from being a view on global tensor\n",
    "            smb = b[bi_range, bj_range].clone()\n",
    "\n",
    "            # print(f'Multiplying a tile i({i, i+tile_width}) j({tile_width*p, tile_width*(p+1)})')\n",
    "            # print(f'Multiplying b tile i({tile_width*p, tile_width*(p+1)}) j({j, j+tile_width})\\n')\n",
    "            for ti, tj in product(range(tile_width), repeat=2):\n",
    "                # dot the tith row of a with the tjth col of b\n",
    "                res[i+ti][j+tj] += sum(sma[ti].flatten() * smb[:, tj].flatten()) # each thread does tile_width muls\n",
    "\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "print(torch.equal(a @ b, tiled_matmul(a, b)))\n",
    "a @ b, tiled_matmul(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
