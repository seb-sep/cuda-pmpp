{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 8, 2, 7],\n",
       "         [4, 0, 6, 9, 7],\n",
       "         [9, 6, 5, 4, 8],\n",
       "         [4, 2, 4, 8, 8],\n",
       "         [9, 3, 5, 6, 5],\n",
       "         [9, 7, 2, 6, 1],\n",
       "         [9, 8, 4, 9, 6]]),\n",
       " tensor([[7, 3, 2, 0, 6, 0, 8, 2, 6],\n",
       "         [6, 4, 6, 5, 6, 9, 4, 5, 5],\n",
       "         [3, 1, 9, 7, 3, 1, 8, 4, 3],\n",
       "         [1, 6, 5, 1, 5, 3, 7, 8, 5],\n",
       "         [6, 5, 2, 0, 4, 3, 5, 8, 5]]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(low=0, high=10, size=(7, 5))\n",
    "b = torch.randint(low=0, high=10, size=(5, 9))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2 3\n",
      "True\n",
      "4 2 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 81,  62, 104,  63,  74,  44, 125, 111,  80],\n",
       "         [ 97, 107, 121,  51, 115,  54, 178, 160, 122],\n",
       "         [166, 120, 135,  69, 157,  95, 204, 164, 159],\n",
       "         [108, 112, 112,  46, 120,  70, 168, 162, 126],\n",
       "         [132, 105, 121,  56, 137,  65, 191, 141, 139],\n",
       "         [123,  98, 110,  55, 136,  86, 163, 117, 130],\n",
       "         [168, 147, 159,  77, 183, 121, 229, 194, 181]]),\n",
       " tensor([[ 81,  62, 104,  63,  74,  44, 125, 111,  80],\n",
       "         [ 97, 107, 121,  51, 115,  54, 178, 160, 122],\n",
       "         [166, 120, 135,  69, 157,  95, 204, 164, 159],\n",
       "         [108, 112, 112,  46, 120,  70, 168, 162, 126],\n",
       "         [132, 105, 121,  56, 137,  65, 191, 141, 139],\n",
       "         [123,  98, 110,  55, 136,  86, 163, 117, 130],\n",
       "         [168, 147, 159,  77, 183, 121, 229, 194, 181]]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "def tiled_matmul(a: torch.Tensor, b: torch.Tensor, tile_width = 2):\n",
    "    assert a.shape[1] == b.shape[0]\n",
    "    res = torch.zeros(a.shape[0], b.shape[1], dtype=int)\n",
    "\n",
    "    # mock shared memory\n",
    "    sma = torch.empty((tile_width, tile_width), dtype=torch.int64)\n",
    "    smb = torch.empty((tile_width, tile_width), dtype=torch.int64)\n",
    "\n",
    "    # number of tiles for outer dims\n",
    "    m_tiles = a.shape[0] // tile_width + (a.shape[0] % tile_width)\n",
    "    n_tiles = b.shape[1] // tile_width + (b.shape[1] % tile_width)\n",
    "    # inner dim tiles\n",
    "    k_tiles = a.shape[1] // tile_width + (a.shape[1] % tile_width)\n",
    "\n",
    "    print(m_tiles, k_tiles, n_tiles)\n",
    "    \n",
    "    # tiling coordinates\n",
    "    for i, j in product(range(0, m_tiles*tile_width, tile_width), range(0, n_tiles*tile_width, tile_width)):\n",
    "        # loop over each phase of tiled matmul\n",
    "        for p in range(k_tiles):\n",
    "\n",
    "            sma.zero_(), smb.zero_()\n",
    "\n",
    "            # each thread loads a value from a and b into shared memory\n",
    "            # a phases across a row, b phases across a column\n",
    "            for ti, tj in product(range(tile_width), repeat=2):\n",
    "                if (i+ti) < a.shape[0] and (tile_width*p+tj) < a.shape[1]:\n",
    "                    sma[ti][tj] = a[i+ti][tile_width*p+tj]\n",
    "                if (tile_width*p+ti) < b.shape[0] and (j+tj) < b.shape[1]:\n",
    "                    smb[ti][tj] = b[tile_width*p+ti][j+tj]\n",
    "\n",
    "            # need to do two separate loops to fully load in shared memory before dot products\n",
    "            for ti, tj in product(range(tile_width), repeat=2):\n",
    "                # dot the tith row of a with the tjth col of b\n",
    "                if (i+ti) < res.shape[0] and (j+tj) < res.shape[1]:\n",
    "                    res[i+ti][j+tj] += sum(sma[ti].flatten() * smb[:, tj].flatten()) # each thread does tile_width muls\n",
    "\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "print(torch.equal(a @ b, tiled_matmul(a, b, tile_width=4)))\n",
    "a @ b, tiled_matmul(a, b, tile_width=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
